# Homework 3: All-Pairs Shortest Path

                                                                    113062573 余侞璇

## Implementation

### 1. Which algorithm do you choose in hw3-1?

使用***pthread***作為主要的threading library, 實作原始的 ***floyd-warshall***。我將 Distance Matrix 中的每條 row 平分給 thread去計算，每個 thread 要執行 chunk 條 row。比如 input n = 10, 有 3 條 thread，則 chunk = 4：

thread 0 處理第 0-3 條 row

thread 1 處理第 4-7 條 row

thread 2 處理第 8-9 條 row

```cpp

chunk = ceil(n / double(ncpus));
......
int id = *((int*)arg);//thred_id
int start = id*chunk;
int end = min((id + 1) * chunk, n);

```

我也有進行 ***Early Stop check***。因為 matrix 的對角線不用計算，當任一 row (or col) 出現無限值時，該 row (or col) 也不用計算。

另外，***floyd-warshall***有 dependency，下一個 stage k 要等上一個 stage k結束才能計算，故在進行下一輪(k)的計算前，我使用了***barrier***將每個 thread 同步。

```cpp

void* cal(void *arg){
  int id = *((int*)arg);//thread_id

  int start = id*chunk;
  int end = min((id + 1) * chunk, n);
  for(int k=0; k<n; k++){
    for(int i=start; i<end; i++){
      if(Dist[i][k] == INF || i == k) continue;
      for(int j = 0; j < n; j++){
        if(Dist[i][k] + Dist[k][j] < Dist[i][j]){
          Dist[i][j] = Dist[i][k] + Dist[k][j];
        }
      }
    }
    pthread_barrier_wait(&barrier);
  }
  return NULL;
}

```

### 2. How do you divide your data in hw3-2, hw3-3?

* hw3-2

    - Padding

        因為本次實作的是 block (tiled) version，vertex 數量不一定能被 blocking factor (B) 整除，此時 kernel 需要去檢查 index out of boundary，造成 branching 增加計算時間。故我使用 padding 加入額外的 vertex，並設置成 INF，將 matrix 邊長變成 B 的倍數。新增的 vertex 與原本的 vertex 沒有連接，所以不會對計算結果造成影響，只要 output 時印出原始的 N*N matrix 即可。

        ex. 若 block dimension 為 (32, 32) ( 即 B = 32 )，matrix 大小為 61 * 61，padding 完會讓 matrix 擴增到 64 * 64。

        ```cpp

        ori_n = n;
        if(n%B != 0) 
            n += (B - (n%B));
        Dist = (int*)malloc(n * n * sizeof(int));

        ```

    - pinned memory

        使用 ***cudaHostRegister***將已經挖好的 cpu matrix memory 先 pin 住，再 load 到 GPU global memory，增加搬移的效率。

        ```cpp

        int *host_D, *dev_D;
        unsigned long matrix = sizeof(int) * n * n;
        cudaHostRegister(Dist, matrix, cudaHostRegisterDefault);
        cudaMalloc(&dev_D, matrix);
        cudaMemcpy(dev_D, Dist, matrix, cudaMemcpyHostToDevice);

        ```
    - bank conflict

        由於一個 block 最多只能用 1024 個 threads，對任何一個 block，我設置其 dimension 為 32 * 32，也就是 warp 的倍數，提高 memory access 的速度，同時也達到 occupancy optimization，減少閒置的 thread。至於 block factor，設置成 64。
        ```cpp

        const int HALF_B 32
        dim3 block_dim(HALF_B, HALF_B);//padding to multiple of 32

        ``` 
        因此每個 thread 要負責計算 4 個 位置。而一個 bank 的大小是 4 bytes，總共有 32 個 bank，我採用以下分法：
        ```
        thread (0, 0) 負責  (0, 0), (32, 0), (0, 32), (32, 32) 四個位置
        thread (1, 1) 負責   (1, 1), (33, 1), (1, 33), (33, 33) 四個位置
        ```
        如此可以保證同個 warp 的 thread 不會同時 access same bank.

    blocked version 就是每個 block 輪流當 pivot block，所以總共要跑 number of block (n / B)次迴圈。我在每次的 iteration 都會呼叫三個 phase 的 kernel function 進行處理。

    ```cpp

    int block_num = (n + B - 1) / B;
    for(int i = 0; i < block_num; i++)
    {
        Block_FW_phase1<<<grid_dim1, block_dim>>>(n, i, dev_D);//block_dim = (32, 32)
        Block_FW_phase2<<<block_num, block_dim>>>(n, i, dev_D);
        Block_FW_phase3<<<grid_dim3, block_dim>>>(n, i, dev_D);
    }

    ```
    - `phase 1`

        ![phase1](https://github.com/109062227/Parallel-Programming/blob/main/hw3/phase1.png?raw=true)

        Phase 1 需要利用 32 * 32 = 1024 個 threads，也就是一個 kernel block，計算一個 64 * 64 的pivot block。每個 thread 負責 4 個 element，並將計算好的值 load 進 ***shared memory***：
        ```cpp
        __shared__ int shared_D[B][B];
        ```
        注意，每個 thread 是同步執行，所以 shared memory 存取完值後，需要呼叫 `__syncthreads()`
        確保所有 thread 執行完畢。首先計算block_offset，也就是當前 pivot block 在 global memory裡面的偏移量，再根據目前的thread計算出他需要計算的四個位置的座標，分別是global_row1, global_row2, global_col1, global_col2。此處的 HALF_B = 32。

        ```cpp
        const int x = threadIdx.x;    
        const int y = threadIdx.y;    
        const int block_offset = B * r; // r 代表第幾round,也就是第幾個pivot block的意思
        const int global_row1 = block_offset + y;
        const int global_row2 = block_offset + y + HALF_B;
        const int global_col1 = block_offset + x;
        const int global_col2 = block_offset + x + HALF_B;
        
        shared_D[y][x] = global_D[global_row1 * n + global_col1];
        shared_D[y + HALF_B][x] = global_D[global_row2 * n + global_col1];
        shared_D[y][x + HALF_B] = global_D[global_row1 * n + global_col2];
        shared_D[y + HALF_B][x + HALF_B] = global_D[global_row2 * n + global_col2];
        __syncthreads();
        ```
        接著進行 floyd warshall algorithm。根據先前提到的每個 thread 負責的位置分配，我是以切 32 (B 的一半)為單位，如此可以讓同個 warp 的 threads 去 access 連續的記憶體，達到 ***Coalesced memory access***。k 從 0 遍歷到 B-1，嘗試每種可能的 vertex 組合，並且每次 iteration 做完都要呼叫 `__syncthreads()`
        ```cpp
        for (int k = 0; k < B; k++) {
            int candidate1 = shared_D[y][k] + shared_D[k][x];
            int candidate2 = shared_D[y + HALF_B][k] + shared_D[k][x];
            int candidate3 = shared_D[y][k] + shared_D[k][x + HALF_B];
            int candidate4 = shared_D[y + HALF_B][k] + shared_D[k][x + HALF_B];

            shared_D[y][x] = min(shared_D[y][x], candidate1);
            shared_D[y + HALF_B][x] = min(shared_D[y + HALF_B][x], candidate2);
            shared_D[y][x + HALF_B] = min(shared_D[y][x + HALF_B], candidate3);
            shared_D[y + HALF_B][x + HALF_B] = min(shared_D[y + HALF_B][x + HALF_B], candidate4);
            __syncthreads();
        }
        ```
        最終再把 shared memory 的 data 寫回 device memory。
        ```cpp
        global_D[global_row1 * n + global_col1] = shared_D[y][x];
        global_D[global_row2 * n + global_col1] = shared_D[y + HALF_B][x];
        global_D[global_row1 * n + global_col2] = shared_D[y][x + HALF_B];
        global_D[global_row2 * n + global_col2] = shared_D[y + HALF_B][x + HALF_B];
        ```
    - `phase2`

        ![phase2](https://github.com/109062227/Parallel-Programming/blob/main/hw3/phase2.png?raw=true)
        
        Phase 2 需要處理 與 phase 1 的 pivot block 同條 row、同條 col 的所有 block。所以我讓 `block_num` 個 kernel block 來平行處理 phase 2，其中 block_num 為 ceil(n / B)，代表 number of kernel block。每個 kernel block 一樣有 32 * 32 個 threads。一個 kernel block 只需要讀取pivot block，與其同row上的一個block (或是同col上的一個block)，即可完成phase2計算。

        首先，對角線的pivot block在phase 1已經計算完，此處可以跳過：
        ```cpp
        const int i = blockIdx.x;
        if (i == r) return; // skip if i == r
        ```
        
        總共開了三個shared memory的陣列，分別存row值，col值，以及對角區塊的值：
        ```cpp
        __shared__ int shared_D[B][B];
        __shared__ int row[B][B];
        __shared__ int col[B][B];
        ```
        因為是shared memory，這三個陣列的值存儲完後，一樣需要呼叫__syncthreads()。接著根據blockIdx.x(i)與 round r(第幾個pivot block)來計算出 row 與 col上面需要的偏移量。此處的對角線偏移來自於(r, r)對應到全局的位置是`(r * B * n) + r * B = r * B * (n + 1)`。下面只有貼了同條col的shared memory 值初始化，row與對角區塊的計算方式同理，只是offset改成對應的即可。
        ```cpp
        const int col_offset = i * B * n + r * B;//i : blockIdx.x, r : round(D[i, r])
        const int row_offset = r * B * n + i * B;//D[r, i]

        const int diagonal_offset = block_offset * (n + 1);//block_offset = r*B

        // col[][] for col fixed,
        col[y][x] = global_D[col_offset + y * n + x];
        col[y + HALF_B][x] = global_D[col_offset + (y + HALF_B) * n + x];
        col[y][x + HALF_B] = global_D[col_offset + y * n + (x + HALF_B)];
        col[y + HALF_B][x + HALF_B] = global_D[col_offset + (y + HALF_B) * n + (x + HALF_B)];
        ......
        __syncthreads()
        ```
        接著進行 floyd warshall algorithm。我使用了 unroll 32來平行化，因為 phase 2計算沒有 stage k dependency問題。選擇只unroll 32 而不是 64的原因是，雖然kernel block大小為 64 * 64，但因為只有32個bank，若unroll 超過32會有bank conflict的問題。row與col上的計算是分開的。
        ```cpp
        #pragma unroll 32
        for (int k = 0; k < B; k++) {
            int candidate_col1 = col[y][k] + shared_D[k][x];
            int candidate_col2 = col[y + HALF_B][k] + shared_D[k][x];
            int candidate_col3 = col[y][k] + shared_D[k][x + HALF_B];
            int candidate_col4 = col[y + HALF_B][k] + shared_D[k][x + HALF_B];

            col[y][x] = min(col[y][x], candidate_col1);
            col[y + HALF_B][x] = min(col[y + HALF_B][x], candidate_col2);
            col[y][x + HALF_B] = min(col[y][x + HALF_B], candidate_col3);
            col[y + HALF_B][x + HALF_B] = min(col[y + HALF_B][x + HALF_B], candidate_col4);

            int candidate_row1 = shared_D[y][k] + row[k][x];
            int candidate_row2 = shared_D[y + HALF_B][k] + row[k][x];
            int candidate_row3 = shared_D[y][k] + row[k][x + HALF_B];
            int candidate_row4 = shared_D[y + HALF_B][k] + row[k][x + HALF_B];

            row[y][x] = min(row[y][x], candidate_row1);
            row[y + HALF_B][x] = min(row[y + HALF_B][x], candidate_row2);
            row[y][x + HALF_B] = min(row[y][x + HALF_B], candidate_row3);
            row[y + HALF_B][x + HALF_B] = min(row[y + HALF_B][x + HALF_B], candidate_row4);
        }
        ```
        最終再把 shared memory 的 data 寫回 device memory。這裡只示範col的儲存。
        ```cpp
        global_D[col_offset + y * n + x] = col[y][x];
        global_D[col_offset + (y + HALF_B) * n + x] = col[y + HALF_B][x];
        global_D[col_offset + y * n + (x + HALF_B)] = col[y][x + HALF_B];
        global_D[col_offset + (y + HALF_B) * n + (x + HALF_B)] = col[y + HALF_B][x + HALF_B];
        ```

    - `phase3`

        ![phase3](https://github.com/109062227/Parallel-Programming/blob/main/hw3/phase3.png?raw=true)
        
        Phase 3 需要處理 phase 1、2 沒有處理到的所有 block ，幾乎接近整個 matrix 的範圍。所以我讓 `block_num * block_num` 個 kernel block 來平行處理 phase 3，每個 kernel block 一樣有 32 * 32 個 threads。一個 kernel block 只需要讀取pivot block，與phase2的row上的一個block + col上的一個block，即可完成phase3計算。
        
        首先，對角線的pivot block在phase 1已經計算完，同條row與同條col的block在phase2也計算過了，故此處可以跳過：
        ```cpp
        const int j = blockIdx.x;
        const int i = blockIdx.y;
        if(i == r && j == r) return;
        ```
        一樣開了三個shared memory的陣列，分別存row值，col值，以及對角區塊的值，要記得呼叫`__syncthreads()`。接著根據blockIdx.x(j), blockIdx.y(i)與 round r(第幾個pivot block)來計算出 row 與 col上面需要的偏移量base_i、base_r。下面只有貼了同條col的shared memory 值初始化，其餘row與diagonal的初始化同理，只是offset改成base_r、base_shared的即可。
        ```cpp
        const int base_i = i * B * n + r * B;//D[i, r]
        const int base_r = r * B * n + j * B;//D[r, j]
        const int base_shared = i * B * n + j * B;//D[i, j]

        // Load columns (from D[i, r] into col)
        col[y][x] = global_D[base_i + y * n + x];
        col[y + HALF_B][x] = global_D[base_i + (y + HALF_B) * n + x];
        col[y][x + HALF_B] = global_D[base_i + y * n + (x + HALF_B)];
        col[y + HALF_B][x + HALF_B] = global_D[base_i + (y + HALF_B) * n + (x + HALF_B)];
        ```
        接著進行 floyd warshall algorithm。這裡也沒有 stage k dependency 的問題，故可使用unroll 32加速效率。
        ```cpp
        #pragma unroll 32
        for(int k = 0; k < B; k++)
        {
            int candidate1 = col[y][k] + row[k][x];
            int candidate2 = col[y + HALF_B][k] + row[k][x];
            int candidate3 = col[y][k] + row[k][x + HALF_B];
            int candidate4 = col[y + HALF_B][k] + row[k][x + HALF_B];

            shared_D[y][x] = min(shared_D[y][x], candidate1);
            shared_D[y + HALF_B][x] = min(shared_D[y + HALF_B][x], candidate2);
            shared_D[y][x + HALF_B] = min(shared_D[y][x + HALF_B], candidate3);
            shared_D[y + HALF_B][x + HALF_B] = min(shared_D[y + HALF_B][x + HALF_B], candidate4);
        }
        ```
        最後要記得把值 load 回 device memory。

* hw3-3

  在multi-GPU中，三個phase的實作細節與hw3-2幾乎相同，所以只稍微介紹剛開始的資料分割，以及loading 最重的phase3之平行化。我將Distance matrix分成上下兩半，一個GPU各自負責一半的位置，phase1與phase2是讓兩個GPU進行相同之計算。我用兩個 thread 來控制GPU，GPU間的溝通也是靠memory copy，thread 0負責上半部的matrix、thread1負責下半部的matrix，他們各自只要copy 一半的 data，故減少了memory copy的時間，達成 load data的平行化。

  如果block數量非偶數，會讓thread 1擁有的block數量再+1，湊成偶數。
  ```cpp
  #pragma omp parallel num_threads(2)
    {
        const int threadId = omp_get_thread_num();
        cudaSetDevice(threadId);

        cudaMalloc(&dev_D[threadId], matrix);

		    int numblock_perThread = block_num / 2;
        const int yOffset = threadId == 1 ? numblock_perThread: 0;//determine上半部or下半部

        // If num of blocks is uneven, we make the second GPU to add "1"
        if(threadId == 1 && (block_num % 2) != 0) numblock_perThread += 1;
        
        cudaMemcpy(dev_D[threadId] + yOffset*B*n, Dist + yOffset*B*n, B * n * sizeof(int) * numblock_perThread, cudaMemcpyHostToDevice);
  ```
  注意，phase3因為兩個gpu各自計算上下部分，所以grid_dim3要變成：
  ```cpp
  dim3 grid_dim3(block_num, numblock_perThread);
  ```
  因為每個phase都會用到pivot row中的某幾個block，但pivot row 上一輪 k 的計算結果只存在另一個gpu中，所以每次開始新的iteration時，上一輪負責的gpu要先把自己處理的pivot row load 回 Host memory，這樣另一個gpu才能再 copy 到 device memory。
  ```cpp
  for(int i = 0; i < block_num; i++)
  {
      // Every thread has its own yOffset
      
      if ((i >= yOffset) && (i < (yOffset + numblock_perThread))) {
          cudaMemcpy(Dist + i * B * n, dev_D[threadId] + i * B * n, B * n * sizeof(int), cudaMemcpyDeviceToHost);
      }
      #pragma omp barrier

      cudaMemcpy(dev_D[threadId] + i * B * n, Dist + i * B * n, B * n * sizeof(int), cudaMemcpyHostToDevice);

      Block_FW_phase1<<<grid_dim1, block_dim>>>(n, i, dev_D[threadId]);
      Block_FW_phase2<<<block_num, block_dim>>>(n, i, dev_D[threadId]);
      Block_FW_phase3<<<grid_dim3, block_dim>>>(n, i, dev_D[threadId], yOffset);
  }
  ```

### 3. What’s your configuration in hw3-2, hw3-3? And why? (e.g. blocking factor, #blocks,#threads)

  * hw3-2

    - blocking factor (B)

      設置成 64，主要是根據shared memory 的大小決定的。若是B太小，會沒辦法充分利用shared memory space，但又不能超過一個block的shared memory 空間 49152 bytes。其中phase2與phase3都用到了3 * B * B大小的shared memory(比phase1多)，故選擇B = 64的話，3 * 64 * 64 * 4 bytes = 49152 bytes，剛好可以用完整個shared memory space。

    - #threads
      
      就像前面提過的，設置block dimension(thread數量) 為 32 * 32，也就是 warp 的倍數，提高 memory access 的速度，同時也達到 occupancy optimization。

    - #blocks

      phase1 : 設置了1個block，因為只需要計算pivot block本人。

      phase2 : block數量為block_num = ceil (n / B)，因為需要處理與 phase 1 的 pivot block 同條    row、同條 col 的所有 block。

      phase3 : block數量為block_num * block_num，因為需要處理 phase 1、2 沒有處理到的所有 block ，幾乎接近整個 matrix 的範圍。
  
  * hw3-3

    - blocking factor & #threads

      與 hw3-2 配置相同。

    - #blocks

      phase1 與 phase2 的 block 數量都與 hw3-2 保持一樣。
      phase3的變成 block_num * (block_num / 2)，因為兩個gpu各自計算上下部分的matrix而已。

### 4. How do you implement the communication in hw3-3

如同上面實作細節提過的，GPU間的溝通是靠memory copy。

## Profiling Results (hw3-2)

testcases : ***p11k1***

我的 biggest kernel 是最下面的 ***phase3*** : 

![phase 3 kernel profile](https://github.com/109062227/Parallel-Programming/blob/main/hw3/3%20phase%20profile.png?raw=true)

可以發現phase3處理的資料量最龐大，因此在occupancy、sm_efficiency、global與shared memory的throughput上，都比 phase 1和phase 2大上一些，充分利用資源。

## Experiment & Analysis

### 1. System Spec

使用課堂提供的 Apollo server。

### 2. Blocking Factor (hw3-2)

testcases : c15.1

*  Integer GOPS

    (三個 kernel function 的 total integer instructions) / (total time(s) * (10^9))

    ![GOPS](https://github.com/109062227/Parallel-Programming/blob/main/hw3/GOPs.png?raw=true)

    隨著 blocking factor 增加，Computing performance 有所進步，也許是因為 fully utilize了shared memory的關係。可以發現，blocking factor太小，會變成花太多時間在memory copy，從而導致較差的效率。當然，blocking factor也不是越大就越好，太大的blocking factor會疊加過多的計算資源在單⼀
    ALU上，無法有效平⾏計算資源，造成效率變差。所以選擇能夠fully utilize shared memory的範圍內，最佳數字是設成64。

* Memory bandwidth

    三個kernel function 的 global load+store bandwidth 的平均

    ![Global_bw](https://github.com/109062227/Parallel-Programming/blob/main/hw3/global_bw.png?raw=true)


    三個kernel function 的 shared load+store bandwidth 的平均

    ![Shared_BW](https://github.com/109062227/Parallel-Programming/blob/main/hw3/shared_bw.png?raw=true)

    可以發現shared memory performance確實隨著blocking factor變大有所提升，但global memory反而呈現相反的趨勢。可能是因為blocking factor越大，每次需要載入更多的資料，導致global memory遇到一些傳輸上的 bottle neck。不過因為整體的執行時間，使用blocking factor = 64還是有優化，故最終仍選擇此數字。

### 3. Optimization (hw3-2)

testcases : c15.1
![Optimization](https://github.com/109062227/Parallel-Programming/blob/main/hw3/optimization2.png?raw=true)

Optimization的方法與做法是先前implementation已經帶過的部分，可以發現shared memory 對runtime影響最大，可見memory存取速度之重要性。使用padding，因為減少了branching，所以在computation time方面有小小提升。調整blocking factor也因為可以fully utilize shared memory而對效能有所幫助。unroll 則是增加指令的並行姓，也間接減少cache miss。Occupancy optimization充分利用了sm 的運算能力，對執行時間的影像肉眼可見一點。

### 4. Weak scalability (hw3-3)

testcase : p23k1, p29k1

因為time complexity是n^3，發現p29k1的數據量約為p23k1的兩倍，故採用這兩筆測資。

![weak scability](https://github.com/109062227/Parallel-Programming/blob/main/hw3/weak_2.png?raw=true)

可以發現 scability 效果並不好，我想也許除了IO time這種無法優化的數值以外，只對phase3做兩個gpu的加速可能也是原因之一，畢竟phase1和2沒有成功利用到兩個GPU的優勢。

### 5. Time Distribution (hw3-2)

![Time Distribution grid](https://github.com/109062227/Parallel-Programming/blob/main/hw3/time2.png?raw=true)

![Time Distribution](https://github.com/109062227/Parallel-Programming/blob/main/hw3/time_distribution.png?raw=true)

io我是自己寫了c++計算時間的function來算，其餘使用nvprof。memory copy time在圖上幾乎可以忽略不計，而io time 與 computing time都會隨著 N 變大而上升，但io time是這次作業無法避免的bottle neck，因此如何提升gpu computing time是重要的目標。通常cuda的計算時間與access memory data之速度有絕對關係，故加速memory access也是作業重要的優化。

### 6. Others

testcases : p23k1

![unroll size](https://github.com/109062227/Parallel-Programming/blob/main/hw3/unroll.png?raw=true)

照理來說，如果對for loop unroll 越多次，執行時間應該要越少。但我發現hw3並非如此，unroll size = 32反而效果優於unroll size = 64，推測是因為如果size設成一個warp的大小，同時進行運算時，較不容易遇見bank conflict，從而產生較佳的結果。

## Experiment on AMD GPU

testcases : c21.1

在hw3-2-amd-judge 時，p17k1以後就會TLE，我就沒有讓他跑下去了。

在hw3-3-amd-judge 時，c06.1與c07.1會TLE。

![amd_nv](https://github.com/109062227/Parallel-Programming/blob/main/hw3/amd.png?raw=true)

可以發現single-GPU中，nvdia的表現就已經高於amd，更不用提multi-GPU的明顯差距。我想可能是因為，nvdia顯卡通常提供了較高的memory bandwidth，並且由於***Tensor Cores***的存在，nvdia在浮點數運算及矩陣運算方面表現也會比amd-GPU來的好。


## Experience & conclusion

### 1. What have you learned from this homework?

    這次花了很多時間在設計memory access，調整出最好的block factor、shared memory存取。原來一份優化的平行程式，除了選用適當演算法之外，了解硬體架構並應用也非常重要，如此才能真正發揮GPU的優勢。

### 2. Feedback

    謝謝教授、助教，你們辛苦了！












