# Homework 2: Mandelbrot Set

                                                                    113062573 余侞璇

## Implementation

### 1. How you implement each of requested versions, especially for the hybrid parallelism

* Pthread version

    * global variable

        在 Mandelbrot Set 計算的公式中，把每次計算 x, y 值時會重複出現的算式預先算好並放進 global variable。

        ```cpp
        y_ulh = (upper - lower) / height;
        x_rlw = (right - left) / width;
        ```
        
        另外，將 image 宣告成 global variable，負責存放 Mandelbrot set 計算輸出的 image。

        ```cpp
        image = (int *)malloc(width * height * sizeof(int));
        ``` 

    * pthread

        設置完所有需要的變數後，再根據現在可用的 cpu 數量 ( ncpus )來 create pthread : 

        ```cpp
        for (int i = 0; i < ncpus; i++)
        {
            pthread_create(&threads[i], NULL, Mandelbrot, NULL);
        }
        ```
        這裡用到的 thread function ***Mandelbrot***會在後面提到細節。
        
        為了讓所有 thread 能夠同步化，最後要再一起執行 join :

        ```cpp
        for (int i = 0; i < ncpus; i++)
        {
            pthread_join(threads[i], NULL);
        }
        ```
        最後再 write png 。


* Hybrid version

    這裡需要使用 MPI + OpenMP，global variable 的宣告大致如 Pthread。比較特別的是，因為有 multi-node 的計算，所以不能像 Pthread 一樣單純用 global variable 來儲存 image。需要透過每個 node 存自己的 ***image***，最後再收集到 ***total_image*** : 
    ```cpp
    image = (int *)malloc(width * chunk * sizeof(int));
    assert(image);

    int *total_image;
    if (mpi_rank == 0)
    {
        total_image = (int *)malloc(mpi_size * width * chunk * sizeof(int));
    }
    ```
    ***chunk***的涵義等等切割資料會詳細說明。

    在 Mandelbrot Set 計算的最外層，加上 ```#pragma omp parallel num_threads(ncpus) shared(image, mpi_rank)``` 啟動一塊平行化的區域，並指定 thread 數量以及 shared variable。實作細節會在後面提到。

    結束計算後，用 ***MPI_Gather***將收集到的 image 總和給 total_image。 MPI_Gather 允許不同 node 回傳不同數量的數據，故此處採用。

    ```cpp
    MPI_Gather(image, width * chunk, MPI_INT, total_image, width * chunk, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Finalize();
    ```

    最後再 write png 。

### 2. How do you partition the task?

兩種 version 都使用 dynamic task assignment 來分配工作( 以 row 為單位 )給每個 thread。我的 work pool 並沒有使用一個 master thread 來管理以及分配工作，反而讓所有 thread 都參與計算，希望能夠最大平行化計算的部分。每個 thread 都是透過 critical section 向 work pool 提出 request，同時 work pool 也會更新狀態。
```cpp
pthread_mutex_lock(&mutex);
j = get_row();
if (j == -1)
{
    pthread_mutex_unlock(&mutex);
    break;
}
pthread_mutex_unlock(&mutex);
```
實際上```get_row()```在兩個版本中稍有不同，下面會介紹。

write png 則都是由一個 thread 負責。

* Pthread version

    ```cpp
    int get_row()
    {
        int ret = -1;
        if (row < height)
        {
            ret = row;
            row++;
        }
        return ret;
    }
    ```
    從第一條 row 開始依序分配，每個 thread 一次只拿一條。***row*** 是 global variable，會初始化為0，用來記錄現在分配到第幾條 row。所有 thread 在 create 後都會進入 while loop 進行計算，透過 critical section 呼叫 get_row()，回傳當前 work pool 的狀態：若是還有剩餘的工作( row < height )，則 return 下一條要分配的 row ，並讓 row++ 以及對分配到的 row 做計算，將 result 寫入 shared memory ***image***；若工作都分配完了( row >= height )，return -1 並 break 出 loop，由 main thread 來 write png。

* Hybrid version

    用 static 的方法來決定每個 process 負責的工作大小，以減少 process 之間的 communication。
    ```cpp
    chunk = ceil(height / (double)mpi_size);
    image = (int *)malloc(width * chunk * sizeof(int));
    ```
    到了 thread 的部分才用 dynamic task assignment。由於 loading 大的工作，可能會出現在相鄰的 row ，為了不讓計算量集中在某幾個 process ，我選擇跳 row 去分配給 thread。***row*** 值會初始化為當前的 rank id，所有 thread 一樣透過 critical section 呼叫 get_row()：若是還有剩餘的工作( row < height )，則 return 下一條要分配的 row ，並讓 row += mpi_size(#process) 以及對分配到的 row 做計算，將 result 寫入 shared memory ***image***；若工作都分配完了( row >= height )，return -1 並 break 出 loop，並用 MPI_Gather 將所有 result 總合起來給 root process，最後由它來 write png。
    ```cpp
    int get_row()
    {
        int ret = -1;
        if (row < height)
        {
            ret = row;
            row += mpi_size; //number of process
        }
        return ret;
    }
    ```
    這是一種 round robin 的方式，可以降低計算壓力，達成 load balancing。舉例來說，假設有 3 個 process，heght 為 9，則：
    * process 0 處理第 0, 3, 6 條 row
    * process 1 處理第 1, 4, 7 條 row
    * process 2 處理第 2, 5, 8 條 row
 
### 3. What technique do you use to reduce execution time and increase scalability?

兩種 version 的 Manddelbrot Set 計算都是根據原始的 code，全部改成 vectorization 做法，採用的是 avx。我這裡是開***兩個 512 bit暫存器***，也就是***一次可以處理 16 個 double***。

```cpp
union AVX
{
    double arr[8];
    __m512d sum;
};
```
Union 內的全部變數都會共享 memory 空間，如果要 load double 時，只需要將 ***arr[0]*** ~ ***arr[7]*** 傳入對應的變數，就可以通過 ***sum*** 的提取，拿到 ```__m512d``` 型別的 data 進行 vectorization。

```cpp
for (int i = 0; i < boundary; i += 16)
{
    AVX x0a, x0b, length_squared_a, length_squared_b, xa, xb, ya, yb;
    int repeat_a[8], repeat_b[8];

    for (int k = 0; k < 8; k++)
    {
        // 第一組 (前 8 個元素)
        x0a.arr[k] = (i + k) * x_rlw + left;
        length_squared_a.arr[k] = 0;
        xa.arr[k] = 0;
        ya.arr[k] = 0;
        repeat_a[k] = 0;

        // 第二組 (後 8 個元素)
        x0b.arr[k] = (i + 8 + k) * x_rlw + left;
        length_squared_b.arr[k] = 0;
        xb.arr[k] = 0;
        yb.arr[k] = 0;
        repeat_b[k] = 0;
    }
    ......
}
```
最外層的 for loop 每次跳 16，代表處理 16 個 double。boundary 是之前提過的global variable ，用來確保計算量是 16 的倍數。這裡分成前後兩組，每組有 8 個 double ，分別對變數們初始化。

```cpp
while (repeats < iters)
{
    // 第一組
    __m512d temp_a = _mm512_add_pd(_mm512_sub_pd(_mm512_mul_pd(xa.sum, xa.sum), _mm512_mul_pd(ya.sum, ya.sum)), x0a.sum);
    ya.sum = _mm512_add_pd(_mm512_mul_pd(_mm512_add_pd(ya.sum, ya.sum), xa.sum), y0_sum);
    xa.sum = temp_a;
    length_squared_a.sum = _mm512_add_pd(_mm512_mul_pd(xa.sum, xa.sum), _mm512_mul_pd(ya.sum, ya.sum));

    ......

    for (int k = 0; k < 8; k++)
    {
        if (length_squared_a.arr[k] >= 4.0 && repeat_a[k] == 0)
        {
            repeat_a[k] = repeats + 1;
        }
        ......
    }

    ++repeats;

    bool complete_a = std::all_of(repeat_a, repeat_a + 8, [](int r)
                                    { return r != 0; });
    ......

    if (complete_a && complete_b)
        break;
}
```

上述只有稍微列出第一組 8 個 channel ( 一個channel 為一個 dobule 占用的空間) 的計算，實際上我總共有兩組 ( 也就是 16 個 channel )同時計算。

此處將終止條件分散在 16 個 channel 上，每次迴圈都會去檢查該 channel 的 length_squared 有無超過 threshold，有的話就讓 repeat_a(b) + 1。最後用 ```all_of``` 檢查是否每個 repeat_a(b) array 都不等於0，若是，則代表完成計算，可以跳出迴圈。

```cpp
if (boundary == boundary2)
{ 
    for (int i = boundary; i < width; ++i)
    {
        double x0 = i * x_rlw + left;
        int repeats = 0;
        double x = 0, y = 0, length_squared = 0;

        while (repeats < iters && length_squared < 4)
        {
            double temp = x * x - y * y + x0;
            y = 2 * x * y + y0;
            x = temp;
            length_squared = x * x + y * y;
            ++repeats;
        }
        image[j * width + i] = repeats;
    }
}
```
如果原本的 width 非 16 的倍數，就用原本的計算方法強制處理剩餘的 pixels。

使用 vectorization 算是我最主要減少時間的方法，其餘小的 Optimization 會在 other efforts 介紹。


#### 4. Other efforts you made in your program

* use ```union```

    Union 內的全部變數都會共享 memory 空間，故比 ```load_d``` 的函式庫，像是 ```_mm512_loadu_pd``` 來獲取變數來的快速，因為省去了 memory copy 的時間。

* 移動 while loop 的終止條件

    原本判斷 repeat_a(b)是否不等於 0 是放在 while 一進來的位置，經實測發現放到 while loop 的最下面，也就是一更新完 repeat_a(b) 的值就做 termination check，大約可以快四秒。

* 增加 vectorization 一次處理的 pixel 數

    一開始只處理 256 bits ( 4 個 double )，時長約 134 秒。改成處理 512 bits 後，(8 個 double )，進步到 96 秒左右。為了更上一層樓，我多開了另一個 512 bit 的暫存器，變成一次處理 16 個 double，瞬間躍升到 78 秒。

* 編譯指令

    hw2a 使用 ```icpc```而不是原本的 g++，因為前者對浮點數計算有做優化，在處理精密浮點數的 case 才不會造成 WA。

    hw2b 原本也想如法炮製使用 ```mpiicpc```而不是原本的 mpicxx。但發現雖然 slow 與 strict 的 case 不會出現 WA，也有達到很好的加速作用，可是 fast case 反而速度下降。推測計算浮點數精度與效能是一種 trade-off，如果要更好的精度，可能就會有額外的開銷。所以最後保留了 mpicxx，搭配使用 ```-ffp-contract=off```關閉浮點數運算中的 FMA ( Fuse Multiply-Add )，使計算不會在多次運算中合併，從而提供較高的精度，消除 WA。

* task assignment

    在 hybrid version 中需要 process 彼此溝通，為了避免 communication overhead，process 所需的工作是採用 static 分配，至於 thread 才使用 dynamic task assignment。從 row 的角度出發，Mandelbrot Set 計算量大的部分大多集中在同個區域，故如果只是單純分一條一條的 row，容易造成某些 process 負擔過大。故採用跳著分配 row 的方式，讓每個 process 負責的 row 分散，大家都會輪流處理到計算量較大的區域。值得注意的是，hw2a 因為每個 thread 都可以自由 request 所有的 region，比較沒有這個問題。hw2b 則是需要 MPI communication，才需要特別留意 process 間工作分配的均勻度。

    當然，拆成 block 來分配工作也是一種可行的方式，不過上述的分配方法已經蠻 balanced ( 實驗部分會 show result )，就沒有再額外嘗試了。

## Experiment & Analysis

### i. Methodology

#### (a). System Spec

課堂提供的 server 設備

#### (b). Performance Metrics

| time | measurement |
|:----------|:----------|
| Preprocessing time | 宣告變數和 allocate 記憶體的時間 |
| IO time | write_png 的時間 |
| Communication time | hybrid version 中，Process 最後回傳 result 做 MPI_Gather 的時間，pthread version 中不需要計算 |
| Computation time | ***All thread*** 開始進行 Mandelbrot Set 計算到 finish 的時間 |

### ii. Plots: Scalability & Load Balancing & Profile

* Experimental Method

    採用 strict36.txt 中的 input。
    Pthread : 一個 node，一個 process，比較 1~6threads per process 的結果
    Hybrid : 一個 node，二個 process，比較 1~6threads per process 的結果

* Time Profile

    * Pthread version

        ![pthread time profile](https://github.com/109062227/Parallel-Programming/blob/main/hw2/pthread_time_profiler.png?raw=true)
    
    * Hybrid version

        ![hybrid time profile](https://github.com/109062227/Parallel-Programming/blob/main/hw2/hybrid_time_profiler.png?raw=true)
        
    可以發現在兩個版本中，computation time 是主要 bottle neck，但隨著 thread 變多而下降。改善的方式，也許可以透過 ```thread affinity```來增加 cache hit rate ，以及減少在不同 core 之間切換的開銷，達到降低更多 computation time 的效果。IO time 佔比第二重，雖然尚不構成 bottle neck，但如果能夠在 write png 時平行化處理，效能上也會有所提升。Communication time ( 紫色 ) 只出現 hybrid version 中，因為只有 MPI_Gather 時需要溝通，所以時間真的非常少，幾乎快要看不見了。至於 preprocessing time ( 黃色 )，在兩種 version 中肉眼幾乎無法辨識，可見其所花費的時間微乎其微。

* Speedup factor

    * Pthread version

        ![pthread speedup factor](https://github.com/109062227/Parallel-Programming/blob/main/hw2/pthread_speedup_factor.png?raw=true)

    * Hybrid version

        ![hybrid speedup factor](https://github.com/109062227/Parallel-Programming/blob/main/hw2/hybrid_speedup_factor.png?raw=true)

    兩者的 speedup factor 都算不錯，幾乎是線性成長。不過 Pthread version 可能因為每個 thread 要輪流 access critical section 去 request 工作，造成了一些 overhead，導致曲線稍微偏離 ideal 結果。改善的方法，也許可以嘗試讓一個 master thread 管理 work pool，其餘 thread 負責計算，減少對 mutex lock 的需求。但要注意少了一個 thread 幫忙計算，是否會導致 computation time 更上升。

    Hybrid version 除了 critical section 的 overhead，還有 MPI communication 的 overhead，故比起 Phtread 又更偏離了一點點，尤其 thread 愈多，影響愈大。改善的方法，可以嘗試讓每個 process 計算完自己的部分然後直接 write file，也就是平行化 write png 的部分，減少 process 之間的 communication。

* Load balancing

    * Pthread version

        | Thread ID | Computation time ( sec ) |
        |:----------:|:----------:|
        | 0 | 2.163350 |
        | 1 | 2.163575 |
        | 2 | 2.164337 |
        | 3 | 2.164601 |
        | 4 | 2.164874 |
        | 5 | 2.165290 |

    * Hybrid version

        | Rank ID | Computation time ( sec ) |
        |:----------:|:----------:|
        | 0 | 0.743679 |
        | 1 | 0.765354 |
        | 2 | 0.773771 |
        | 3 | 0.771537 |
        | 4 | 0.770180 |
        | 5 | 0.770388 |
    
    首先測試 Pthread 的 dynamic task assignment，發現工作分配的蠻平均，每個 thread 之間的計算時間差異非常小。故到了 Hybrid 在 thread 計算時也沿用 dynamic task assignment，而 process 間則採用跳 row 分配，這裡只有測試了 process 的 computation time ( 因為 thread 已經在剛才的版本驗證過 )，每個 process 所耗的計算時間也幾乎相同。些微的差距可能來自，當 number of process 不能整除 row 時，rank id 小的 process 也許會比其餘 process 多處理一點工作。

* Other Experiment
    
    ![#number process * #thread](https://github.com/109062227/Parallel-Programming/blob/main/hw2/comm_time.png?raw=true)

    在 hw2b 時，可以嘗試不同的 process 與 thread 組合，會影響最終的 result。假設有 6 個 threads，則有 ```1 process * 6 threads ```、```2 processes * 3 threads ```、```3 processes * 2 threads ``` 與 ```6 processes * 1 thread``` 四種搭配，上圖是我做的一個實驗，展現不同組合的 communication time 之結果。其中 x 軸為***process 數量 * 每個 process 有多少個 thread***，比如 ```3*2```代表 3 個 process，每個 process 有 2 個 threads。

    觀察圖片可以發現，如果想避免 communication time 呈現指數成長，使用少一點的 process 搭配 多一點的 threads，會是比較好的選擇。

## Conclusion

這次作業我將精力集中在減少 computation time 這個 bottle neck，研究 vectorization 真的花了非常多時間。另外，良好的工作分配也很重要，會直接影響到整體的效能，所以我也測試了不少方法，最後選擇簡單易實作的跳 row 分配，達到不錯的 work balanced，整體的 speedup factor 表現也尚可。當然還有不少能夠改進的地方，computation time 應該能夠再更壓縮，write png 平行化降低 IO time 也是可以努力的空間。最後，謝謝教授、助教，你們辛苦了！
