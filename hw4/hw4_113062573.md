# Homework 4: FlashAttention

                                                                    113062573 余侞璇

## Implementation

### 1. Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (ℓ and 𝑚) were calculated.

首先在device端挖好需要的d_Q, d_K, d_V, d_O等 global memroy，大小都是B * N * d，然後再把host的值複製過來。以下只示範Q：
```cpp
cudaMalloc((void**)&d_Q, B * N * d * sizeof(float));
cudaMemcpy(d_Q, Q, B * N * d * sizeof(float), cudaMemcpyHostToDevice);
```

* Matrix Blocking

    對Q來說，我讓每個 batch B 把 自己的 N 分成 Br 大小的 block, 然後讓每個 block 內的 Br 個 threads來處理。所有 block 都是 concurrent 執行的。其中Br = 32。
    ```cpp
    dim3 grid_dim((N + Br - 1)/Br, 1, B);  // Blocks for N dimension and batches
    dim3 block_dim(Br);  // Threads per block

    const float softmax_scale = 1.0 / sqrt(d);

    flash_attention<<<grid_dim, block_dim>>>(d_Q, d_K, d_V, d_O, d, N, softmax_scale);
    ```
    接著進入到 kernel function 後，再將每個 block 拆分成更細的 TILE_SIZE( = Bc ) 大小的 `sub block (tile)` 來處理。
    ```cpp
     for (int j = 0; j < N; j += TILE_SIZE) {
        int remaining = min(TILE_SIZE, N - j);
        ......
    ```
    設計良好的matrix blocking方法可以減少對global memory 的 access，提升overal  memory bandwidth 的utilization。
    
*  SRAM usage

    tile_k和tile_v的大小在sequential版本裡面是bc * d，因為d的範圍是[32, 64]，故此處統一乘上64，TILE_SIZE 就是 bc，代表一次 load ***一個 sub block 的 data***進 tile_k (or tile_v)。qi和oi就跟sequential裡面一樣挖 Br * 64(=d)，代表一次 load ***一個 thread 處理的 data 量***進 qi(or oi)。sij、pij按照sequential版本挖了Br * Bc(=TILE_SIZE)，儲存attention score computation的intermediate result與softmax probabilities。li, mi則是挖 Br，儲存stable softmax computation 的 max values 和 scaling factors。

    Br設成32，TILE_SIZE設成16，不能設太大的數字，否則會超過shared memory的可用limit。適當的使用shared memory，可以加快data access的速度，提升不少效率。
    ```cpp
    const int TILE_SIZE = Br/2;
    __shared__ float tile_k[TILE_SIZE * 64];
    __shared__ float tile_v[TILE_SIZE * 64];
    __shared__ float qi[Br * 64];
    __shared__ float oi[Br * 64];
    __shared__ float li[Br];
    __shared__ float mi[Br];
    __shared__ float sij[Br * TILE_SIZE];
    __shared__ float pij[Br * TILE_SIZE];
    ```
* Initialization

    每個 kernel block都有自己的這些shared memory，所以每次進kernel function都需要先初始化他們。mi與li都是 load #thread的大小，所以根據他現在的threadIdx.x(tx)即可做更新。qi與oi因為是load #threads * d的大小，所以需要根據當前tx與第幾個d才能得到對應global中同個位子的q值。

    此處q的index方法來自於，q的大小為B * N * d，所以需要計算出是第幾個batch、N裡面的第幾個kernel block再搭配thread位置而得。

    因為是更新shared memory的值，所以要記得加上`__syncthreads()`確保所有thread都已完成。

    ```cpp
    int tx = threadIdx.x;
    int bx = blockIdx.x;
    int batch = blockIdx.z;
    int global_row = bx * blockDim.x + tx;
    int batch_offset = batch * N * d;

    if (tx < Br) {
        mi[tx] = FLT_MIN;
        li[tx] = 0.0f;
        for (int x = 0; x < d; x++) {
            qi[tx * d + x] = q[batch_offset + global_row * d + x];
            oi[tx * d + x] = 0.0f;  // 初始化輸出
        }
    }
    __syncthreads();
    ```
    接著是前面有提過的，每個kernel block會再拆成TILE_SIZE大小的sub block ( 以下簡稱 ***tile*** ) 進行內部運算。***remaining***是在處理邊界值的情況(如果N不是TILE_SIZE的倍數)，再來根據當前對應到global的batch位置 + 所處第幾個kernel block + thread位置，將K與V的值 load 給 tile_k和tile_v。最後要記得加上__syncthreads()確保threads工作完成。
    ```cpp
    for (int j = 0; j < N; j += TILE_SIZE) {
        int remaining = min(TILE_SIZE, N - j);
        
        // 載入K和V
        if (tx < TILE_SIZE && (j + tx) < N) {
            for (int x = 0; x < d; x++) {
                tile_k[tx * d + x] = k[batch_offset + (j + tx) * d + x];
                tile_v[tx * d + x] = v[batch_offset + (j + tx) * d + x];
            }
        }
        __syncthreads();
    ......
    }
    ```
    可以發現，對於 q, k, v, o等矩陣的讀寫，都會根據threadIdx按照順序讀寫，這樣的操作可以確保threads按順數去access memory data，實現了 coalesced memory access。

    到目前為止，已經完成所有陣列的初始化與介紹。大致來說，***一個kernel block會負責Br行的qi，也就是一條thread會負責一行的qi，而每行 qi 的計算又會被分成多個 tile，每個tile包含一部分的k與v***。

* QKDotAndScalar & RowMax

    開始計算每個 tile 的sij，先用qi與tile_k^T做dot，再用1/(根號d)當作scaling factor，得到的結果就是sij。值得注意的是，算完每個sij我都會更新目前此 tile 內的最大值 max_val。
    ```cpp
    // 計算S_ij (QK^T)
    float max_val = FLT_MIN;
    for (int t = 0; t < remaining; t++) {
        float dot = 0.0f;
        for (int x = 0; x < d; x++) {
            dot += qi[tx * d + x] * tile_k[t * d + x];
        }
        sij[tx * TILE_SIZE + t] = dot * scale;
        max_val = _max(max_val, sij[tx * TILE_SIZE + t]);
    }
    ```
    拿剛算完的tile內最大值，去與當前thread累積計算的tile的最大值做比較與更新，得到 mi_new 提供穩定指數的數值計算。

    每個 tile 都會生成一部分 softmax 的指數值，因此必須累積所有 tile 的指數和作為最終 softmax 的分母。li 為逐步更新的 ***scaling factor***,，就是當前 thread 處理的所有 tile 的累積指數和。此處基於新的最大值 mi_new 來調整指數和，搭配 cuda 內建的 `__expf()`，該函數能夠保持數值計算的穩定性與準確性，提高硬體計算的efficiency。
    ```cpp
    float mi_new = _max(mi[tx], max_val);
    float li_new = li[tx] * __expf(mi[tx] - mi_new);
    ```
    其實這個演算法就是用***log-sum-exp scaling***來保持softmax computation時的numerical stability。所以每條thread做exponentiation時最大值 mi 都會被追蹤，以免 overflow。Scaling factor li 則是在 tile 間不斷 iteratively update，確保所有tile的contribution都能被正確normalized。

* Softmax Computation

    利用當前儲存在thread tx的位置t的sij結果，去與當前thread的最大值mi_new相減再取指數，確保數值穩定，再存回pij。pij是當前位置的指數權重，為softmax的指數部分。li_new會不斷更新，將所有指數權重加起來，成為softmax分母的一部分，用來normalize probabilities。
    ```cpp
    // 計算softmax分母並更新注意力權重
    for (int t = 0; t < remaining; t++) {
        pij[tx * TILE_SIZE + t] = __expf(sij[tx * TILE_SIZE + t] - mi_new);
        li_new += pij[tx * TILE_SIZE + t];
    }
    ```

* Updating Output

    使用存儲在 pij 中的probability，對每個 v 更新 o : 根據先前計算出的scaling factor (mi 和 li)，累加加權的 v 的縮放和，從而更新 o 。
    ```cpp
    float scale_old = __expf(mi[tx] - mi_new) * li[tx] / li_new;
    float scale_new = 1.0f / li_new;
    
    for (int x = 0; x < d; x++) {
        float new_val = 0.0f;
        for (int t = 0; t < remaining; t++) {
            new_val += pij[tx * TILE_SIZE + t] * tile_v[t * d + x];
        }
        oi[tx * d + x] = oi[tx * d + x] * scale_old + new_val * scale_new;
    }
    
    mi[tx] = mi_new;
    li[tx] = li_new;
    ```
### 2. Explain how matrices Q, K, and V are divided into blocks and processed in parallel.

 如同第一部分實作有提到的：Q, K, V 大小為 (N, d)，其中 Q 會根據sequence legth N 被劃分成大小為 `Br * d`的blocks，每個 block有Br條threads可以 concurrent 處理Br條row，每條row有d個element。每個batch都按照此方式分塊，且batch之間是獨立的。

 K與V則是在每個block內，被劃分成大小為`TILE_SIZE * d`的tile，其中TILE_SIZE就是演算法中的B_c。一個block負責Br條qi的計算，每條qi又分成多個tile，每個tile就是剛才提到K與V被劃分的大小。

### 3. Describe how you chose the block sizes B_r and B_c and why.

總共用到的shared memory量為kernel function中列出來的 tile_k, tile_v, qi, oi, li, mi, sij, pij之總和，一共`2 * Bc * 64 + 2 * br * 64 + 2 * br + 2 * br * bc`，其中Br = 32, 而Bc = Br / 2。

會將Bc設成Br的一半，是為了tiling計算時能減少每個tile所需的shared memory，允許更多block同時在一個SM上面執行。另外，少量的bc可以提高數據訪問的局部性，並且減少__syncthreads()時需要同步的thread數量。

Br則是設成32，與warp size對齊，達到occupancy optimization的效果。如果設成64，根據上面列出的shared memory 使用總和計算，會超過shared memory可以使用的記憶體限制。

### 4.  Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.

* grid_dim

    ```cpp
    dim3 grid_dim((N + Br - 1)/Br, 1, B);
    ```

* #threads per block

    ```cpp
    dim3 block_dim(Br);  // Threads per block, Br = 32
    ```

* shared memory allocation

    ```cpp
    const int TILE_SIZE = Br/2;
    __shared__ float tile_k[TILE_SIZE * 64];
    __shared__ float tile_v[TILE_SIZE * 64];
    __shared__ float qi[Br * 64];
    __shared__ float oi[Br * 64];
    __shared__ float li[Br];
    __shared__ float mi[Br];
    __shared__ float sij[Br * TILE_SIZE];
    __shared__ float pij[Br * TILE_SIZE];
    ```
### 5. Justify your choices and how they relate to the blocking factors and the SRAM size.

配置的原因前面就提過了，此處不贅述。並且 br 跟 bc 的設置帶入shared memory 總使用量`2 * Bc * 64 + 2 * br * 64 + 2 * br + 2 * br * bc` = 28928 bytes，不會超過一個block的shared memory 限制 (49152 bytes)。

## Profiling Results

testcases : t20

![profile](https://github.com/109062227/Parallel-Programming/blob/main/hw4/profile.png?raw=true)

可以看到sm_effiency高達90多趴，代表occupancy optimization的部分很成功。

## Experiment&Analysis

### 1.  System Spec

使用課堂提供的Apollo server.

### 2. Optimization

testcases : t20

![optimization](https://github.com/109062227/Parallel-Programming/blob/main/hw4/optimization.png?raw=true)

Optimization 的 method 和做法如前面 implementation 中所提，對速度影響最大的是shared memory的使用，可見memory存取速度對整個
runtime 的影響之大。Coalesced memory因為在每個kernel block 讀取Q、K、V矩陣時，使用了連續的memory access pattern，所以對runtime也有些微的幫助。Occupancy Optimization則是因為thread設置成warp size，以及合理的block、tile配置，成功最大化了每個SM的運算能力(profiling中可以發現sm_efficiency高達90幾趴)，提升執行效率。

### Others

* Br設置

    testcases : t20

    備註：為了能夠符合shared memory使用限制，將bc暫時調成br/8，這樣才能跑br = 64的case。

    ![br](https://github.com/109062227/Parallel-Programming/blob/main/hw4/diff_br.png?raw=true)

    可以發現br = 64時表現較差，可能是因為我沒有handle bank conflict，所以blocking factor調越大，threads之間可能會更頻繁去access同個bank，造成conflict。

* Bc設置

    testcases : t20

    ![bc](https://github.com/109062227/Parallel-Programming/blob/main/hw4/diff_bc.png?raw=true)

    嘗試調整bc的大小，原始設定是br / 2。x軸代表bc改成br/1, br/2, br/4...依此類推，發現較大的bc會造成表現較差，bc = br/16時尤其明顯。因為切成越多塊tile，可能會導致數據在global 與 shared memory之間頻繁交換，即memory access不高效。另外，較大的bc可能也導致大量同步化等待時間，造成效率差。

    bc = br / 1其實時間稍為比br / 2低了一點，但考量到tile如果切小一點，可以增加sm_efficiency，故最後仍選擇使用br / 2當作bc。

* Different version implementation

    testcases : t22

    ![version](https://github.com/109062227/Parallel-Programming/blob/main/hw4/version.png?raw=true)

    - old version: grid_dim(B, 1), block_dim(32)的版本，可能因為在設計kernel block的部分不夠平行化，只想到用batch的數量來分割。但這會導致在B不大，N很大的時候產生TLE。如我使用的t22 testcases, 另外t26~t30也都會有TLE的狀況。

    - new version: 最後實作的方法，也就是前面有介紹到的grid_dim與block_dim切法。

## Experience & conclusion

### 1. What have you learned from this homework?

原來設計grid_dim與block_dim需要這麼多巧思，真的要注意很多眉角，切得不好可能會TLE。我也對於GPU的撰寫有更深刻的認識，各種優化方法自己實作後比上課單純聽講更有概念。

### 2. Feedback

謝謝教授、助教，你們辛苦了！