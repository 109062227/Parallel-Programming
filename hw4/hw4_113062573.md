# Homework 4: FlashAttention

                                                                    113062573 ä½™ä¾ç’‡

## Implementation

### 1. Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (â„“ and ğ‘š) were calculated.

é¦–å…ˆåœ¨deviceç«¯æŒ–å¥½éœ€è¦çš„d_Q, d_K, d_V, d_Oç­‰ global memroyï¼Œå¤§å°éƒ½æ˜¯B * N * dï¼Œç„¶å¾Œå†æŠŠhostçš„å€¼è¤‡è£½éä¾†ã€‚ä»¥ä¸‹åªç¤ºç¯„Qï¼š
```cpp
cudaMalloc((void**)&d_Q, B * N * d * sizeof(float));
cudaMemcpy(d_Q, Q, B * N * d * sizeof(float), cudaMemcpyHostToDevice);
```

* Matrix Blocking

    å°Qä¾†èªªï¼Œæˆ‘è®“æ¯å€‹ batch B æŠŠ è‡ªå·±çš„ N åˆ†æˆ Br å¤§å°çš„ block, ç„¶å¾Œè®“æ¯å€‹ block å…§çš„ Br å€‹ threadsä¾†è™•ç†ã€‚æ‰€æœ‰ block éƒ½æ˜¯ concurrent åŸ·è¡Œçš„ã€‚å…¶ä¸­Br = 32ã€‚
    ```cpp
    dim3 grid_dim((N + Br - 1)/Br, 1, B);  // Blocks for N dimension and batches
    dim3 block_dim(Br);  // Threads per block

    const float softmax_scale = 1.0 / sqrt(d);

    flash_attention<<<grid_dim, block_dim>>>(d_Q, d_K, d_V, d_O, d, N, softmax_scale);
    ```
    æ¥è‘—é€²å…¥åˆ° kernel function å¾Œï¼Œå†å°‡æ¯å€‹ block æ‹†åˆ†æˆæ›´ç´°çš„ TILE_SIZE( = Bc ) å¤§å°çš„ `sub block (tile)` ä¾†è™•ç†ã€‚
    ```cpp
     for (int j = 0; j < N; j += TILE_SIZE) {
        int remaining = min(TILE_SIZE, N - j);
        ......
    ```
    è¨­è¨ˆè‰¯å¥½çš„matrix blockingæ–¹æ³•å¯ä»¥æ¸›å°‘å°global memory çš„ accessï¼Œæå‡overal  memory bandwidth çš„utilizationã€‚
    
*  SRAM usage

    tile_kå’Œtile_vçš„å¤§å°åœ¨sequentialç‰ˆæœ¬è£¡é¢æ˜¯bc * dï¼Œå› ç‚ºdçš„ç¯„åœæ˜¯[32, 64]ï¼Œæ•…æ­¤è™•çµ±ä¸€ä¹˜ä¸Š64ï¼ŒTILE_SIZE å°±æ˜¯ bcï¼Œä»£è¡¨ä¸€æ¬¡ load ***ä¸€å€‹ sub block çš„ data***é€² tile_k (or tile_v)ã€‚qiå’Œoiå°±è·Ÿsequentialè£¡é¢ä¸€æ¨£æŒ– Br * 64(=d)ï¼Œä»£è¡¨ä¸€æ¬¡ load ***ä¸€å€‹ thread è™•ç†çš„ data é‡***é€² qi(or oi)ã€‚sijã€pijæŒ‰ç…§sequentialç‰ˆæœ¬æŒ–äº†Br * Bc(=TILE_SIZE)ï¼Œå„²å­˜attention score computationçš„intermediate resultèˆ‡softmax probabilitiesã€‚li, miå‰‡æ˜¯æŒ– Brï¼Œå„²å­˜stable softmax computation çš„ max values å’Œ scaling factorsã€‚

    Brè¨­æˆ32ï¼ŒTILE_SIZEè¨­æˆ16ï¼Œä¸èƒ½è¨­å¤ªå¤§çš„æ•¸å­—ï¼Œå¦å‰‡æœƒè¶…éshared memoryçš„å¯ç”¨limitã€‚é©ç•¶çš„ä½¿ç”¨shared memoryï¼Œå¯ä»¥åŠ å¿«data accessçš„é€Ÿåº¦ï¼Œæå‡ä¸å°‘æ•ˆç‡ã€‚
    ```cpp
    const int TILE_SIZE = Br/2;
    __shared__ float tile_k[TILE_SIZE * 64];
    __shared__ float tile_v[TILE_SIZE * 64];
    __shared__ float qi[Br * 64];
    __shared__ float oi[Br * 64];
    __shared__ float li[Br];
    __shared__ float mi[Br];
    __shared__ float sij[Br * TILE_SIZE];
    __shared__ float pij[Br * TILE_SIZE];
    ```
* Initialization

    æ¯å€‹ kernel blockéƒ½æœ‰è‡ªå·±çš„é€™äº›shared memoryï¼Œæ‰€ä»¥æ¯æ¬¡é€²kernel functionéƒ½éœ€è¦å…ˆåˆå§‹åŒ–ä»–å€‘ã€‚mièˆ‡liéƒ½æ˜¯ load #threadçš„å¤§å°ï¼Œæ‰€ä»¥æ ¹æ“šä»–ç¾åœ¨çš„threadIdx.x(tx)å³å¯åšæ›´æ–°ã€‚qièˆ‡oiå› ç‚ºæ˜¯load #threads * dçš„å¤§å°ï¼Œæ‰€ä»¥éœ€è¦æ ¹æ“šç•¶å‰txèˆ‡ç¬¬å¹¾å€‹dæ‰èƒ½å¾—åˆ°å°æ‡‰globalä¸­åŒå€‹ä½å­çš„qå€¼ã€‚

    æ­¤è™•qçš„indexæ–¹æ³•ä¾†è‡ªæ–¼ï¼Œqçš„å¤§å°ç‚ºB * N * dï¼Œæ‰€ä»¥éœ€è¦è¨ˆç®—å‡ºæ˜¯ç¬¬å¹¾å€‹batchã€Nè£¡é¢çš„ç¬¬å¹¾å€‹kernel blockå†æ­é…threadä½ç½®è€Œå¾—ã€‚

    å› ç‚ºæ˜¯æ›´æ–°shared memoryçš„å€¼ï¼Œæ‰€ä»¥è¦è¨˜å¾—åŠ ä¸Š`__syncthreads()`ç¢ºä¿æ‰€æœ‰threadéƒ½å·²å®Œæˆã€‚

    ```cpp
    int tx = threadIdx.x;
    int bx = blockIdx.x;
    int batch = blockIdx.z;
    int global_row = bx * blockDim.x + tx;
    int batch_offset = batch * N * d;

    if (tx < Br) {
        mi[tx] = FLT_MIN;
        li[tx] = 0.0f;
        for (int x = 0; x < d; x++) {
            qi[tx * d + x] = q[batch_offset + global_row * d + x];
            oi[tx * d + x] = 0.0f;  // åˆå§‹åŒ–è¼¸å‡º
        }
    }
    __syncthreads();
    ```
    æ¥è‘—æ˜¯å‰é¢æœ‰æéçš„ï¼Œæ¯å€‹kernel blockæœƒå†æ‹†æˆTILE_SIZEå¤§å°çš„sub block ( ä»¥ä¸‹ç°¡ç¨± ***tile*** ) é€²è¡Œå…§éƒ¨é‹ç®—ã€‚***remaining***æ˜¯åœ¨è™•ç†é‚Šç•Œå€¼çš„æƒ…æ³(å¦‚æœNä¸æ˜¯TILE_SIZEçš„å€æ•¸)ï¼Œå†ä¾†æ ¹æ“šç•¶å‰å°æ‡‰åˆ°globalçš„batchä½ç½® + æ‰€è™•ç¬¬å¹¾å€‹kernel block + threadä½ç½®ï¼Œå°‡Kèˆ‡Vçš„å€¼ load çµ¦ tile_kå’Œtile_vã€‚æœ€å¾Œè¦è¨˜å¾—åŠ ä¸Š__syncthreads()ç¢ºä¿threadså·¥ä½œå®Œæˆã€‚
    ```cpp
    for (int j = 0; j < N; j += TILE_SIZE) {
        int remaining = min(TILE_SIZE, N - j);
        
        // è¼‰å…¥Kå’ŒV
        if (tx < TILE_SIZE && (j + tx) < N) {
            for (int x = 0; x < d; x++) {
                tile_k[tx * d + x] = k[batch_offset + (j + tx) * d + x];
                tile_v[tx * d + x] = v[batch_offset + (j + tx) * d + x];
            }
        }
        __syncthreads();
    ......
    }
    ```
    å¯ä»¥ç™¼ç¾ï¼Œå°æ–¼ q, k, v, oç­‰çŸ©é™£çš„è®€å¯«ï¼Œéƒ½æœƒæ ¹æ“šthreadIdxæŒ‰ç…§é †åºè®€å¯«ï¼Œé€™æ¨£çš„æ“ä½œå¯ä»¥ç¢ºä¿threadsæŒ‰é †æ•¸å»access memory dataï¼Œå¯¦ç¾äº† coalesced memory accessã€‚

    åˆ°ç›®å‰ç‚ºæ­¢ï¼Œå·²ç¶“å®Œæˆæ‰€æœ‰é™£åˆ—çš„åˆå§‹åŒ–èˆ‡ä»‹ç´¹ã€‚å¤§è‡´ä¾†èªªï¼Œ***ä¸€å€‹kernel blockæœƒè² è²¬Brè¡Œçš„qiï¼Œä¹Ÿå°±æ˜¯ä¸€æ¢threadæœƒè² è²¬ä¸€è¡Œçš„qiï¼Œè€Œæ¯è¡Œ qi çš„è¨ˆç®—åˆæœƒè¢«åˆ†æˆå¤šå€‹ tileï¼Œæ¯å€‹tileåŒ…å«ä¸€éƒ¨åˆ†çš„kèˆ‡v***ã€‚

* QKDotAndScalar & RowMax

    é–‹å§‹è¨ˆç®—æ¯å€‹ tile çš„sijï¼Œå…ˆç”¨qièˆ‡tile_k^Tåšdotï¼Œå†ç”¨1/(æ ¹è™Ÿd)ç•¶ä½œscaling factorï¼Œå¾—åˆ°çš„çµæœå°±æ˜¯sijã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç®—å®Œæ¯å€‹sijæˆ‘éƒ½æœƒæ›´æ–°ç›®å‰æ­¤ tile å…§çš„æœ€å¤§å€¼ max_valã€‚
    ```cpp
    // è¨ˆç®—S_ij (QK^T)
    float max_val = FLT_MIN;
    for (int t = 0; t < remaining; t++) {
        float dot = 0.0f;
        for (int x = 0; x < d; x++) {
            dot += qi[tx * d + x] * tile_k[t * d + x];
        }
        sij[tx * TILE_SIZE + t] = dot * scale;
        max_val = _max(max_val, sij[tx * TILE_SIZE + t]);
    }
    ```
    æ‹¿å‰›ç®—å®Œçš„tileå…§æœ€å¤§å€¼ï¼Œå»èˆ‡ç•¶å‰threadç´¯ç©è¨ˆç®—çš„tileçš„æœ€å¤§å€¼åšæ¯”è¼ƒèˆ‡æ›´æ–°ï¼Œå¾—åˆ° mi_new æä¾›ç©©å®šæŒ‡æ•¸çš„æ•¸å€¼è¨ˆç®—ã€‚

    æ¯å€‹ tile éƒ½æœƒç”Ÿæˆä¸€éƒ¨åˆ† softmax çš„æŒ‡æ•¸å€¼ï¼Œå› æ­¤å¿…é ˆç´¯ç©æ‰€æœ‰ tile çš„æŒ‡æ•¸å’Œä½œç‚ºæœ€çµ‚ softmax çš„åˆ†æ¯ã€‚li ç‚ºé€æ­¥æ›´æ–°çš„ ***scaling factor***,ï¼Œå°±æ˜¯ç•¶å‰ thread è™•ç†çš„æ‰€æœ‰ tile çš„ç´¯ç©æŒ‡æ•¸å’Œã€‚æ­¤è™•åŸºæ–¼æ–°çš„æœ€å¤§å€¼ mi_new ä¾†èª¿æ•´æŒ‡æ•¸å’Œï¼Œæ­é… cuda å…§å»ºçš„ `__expf()`ï¼Œè©²å‡½æ•¸èƒ½å¤ ä¿æŒæ•¸å€¼è¨ˆç®—çš„ç©©å®šæ€§èˆ‡æº–ç¢ºæ€§ï¼Œæé«˜ç¡¬é«”è¨ˆç®—çš„efficiencyã€‚
    ```cpp
    float mi_new = _max(mi[tx], max_val);
    float li_new = li[tx] * __expf(mi[tx] - mi_new);
    ```
    å…¶å¯¦é€™å€‹æ¼”ç®—æ³•å°±æ˜¯ç”¨***log-sum-exp scaling***ä¾†ä¿æŒsoftmax computationæ™‚çš„numerical stabilityã€‚æ‰€ä»¥æ¯æ¢threadåšexponentiationæ™‚æœ€å¤§å€¼ mi éƒ½æœƒè¢«è¿½è¹¤ï¼Œä»¥å… overflowã€‚Scaling factor li å‰‡æ˜¯åœ¨ tile é–“ä¸æ–· iteratively updateï¼Œç¢ºä¿æ‰€æœ‰tileçš„contributionéƒ½èƒ½è¢«æ­£ç¢ºnormalizedã€‚

* Softmax Computation

    åˆ©ç”¨ç•¶å‰å„²å­˜åœ¨thread txçš„ä½ç½®tçš„sijçµæœï¼Œå»èˆ‡ç•¶å‰threadçš„æœ€å¤§å€¼mi_newç›¸æ¸›å†å–æŒ‡æ•¸ï¼Œç¢ºä¿æ•¸å€¼ç©©å®šï¼Œå†å­˜å›pijã€‚pijæ˜¯ç•¶å‰ä½ç½®çš„æŒ‡æ•¸æ¬Šé‡ï¼Œç‚ºsoftmaxçš„æŒ‡æ•¸éƒ¨åˆ†ã€‚li_newæœƒä¸æ–·æ›´æ–°ï¼Œå°‡æ‰€æœ‰æŒ‡æ•¸æ¬Šé‡åŠ èµ·ä¾†ï¼Œæˆç‚ºsoftmaxåˆ†æ¯çš„ä¸€éƒ¨åˆ†ï¼Œç”¨ä¾†normalize probabilitiesã€‚
    ```cpp
    // è¨ˆç®—softmaxåˆ†æ¯ä¸¦æ›´æ–°æ³¨æ„åŠ›æ¬Šé‡
    for (int t = 0; t < remaining; t++) {
        pij[tx * TILE_SIZE + t] = __expf(sij[tx * TILE_SIZE + t] - mi_new);
        li_new += pij[tx * TILE_SIZE + t];
    }
    ```

* Updating Output

    ä½¿ç”¨å­˜å„²åœ¨ pij ä¸­çš„probabilityï¼Œå°æ¯å€‹ v æ›´æ–° o : æ ¹æ“šå…ˆå‰è¨ˆç®—å‡ºçš„scaling factor (mi å’Œ li)ï¼Œç´¯åŠ åŠ æ¬Šçš„ v çš„ç¸®æ”¾å’Œï¼Œå¾è€Œæ›´æ–° o ã€‚
    ```cpp
    float scale_old = __expf(mi[tx] - mi_new) * li[tx] / li_new;
    float scale_new = 1.0f / li_new;
    
    for (int x = 0; x < d; x++) {
        float new_val = 0.0f;
        for (int t = 0; t < remaining; t++) {
            new_val += pij[tx * TILE_SIZE + t] * tile_v[t * d + x];
        }
        oi[tx * d + x] = oi[tx * d + x] * scale_old + new_val * scale_new;
    }
    
    mi[tx] = mi_new;
    li[tx] = li_new;
    ```
### 2. Explain how matrices Q, K, and V are divided into blocks and processed in parallel.

 å¦‚åŒç¬¬ä¸€éƒ¨åˆ†å¯¦ä½œæœ‰æåˆ°çš„ï¼šQ, K, V å¤§å°ç‚º (N, d)ï¼Œå…¶ä¸­ Q æœƒæ ¹æ“šsequence legth N è¢«åŠƒåˆ†æˆå¤§å°ç‚º `Br * d`çš„blocksï¼Œæ¯å€‹ blockæœ‰Bræ¢threadså¯ä»¥ concurrent è™•ç†Bræ¢rowï¼Œæ¯æ¢rowæœ‰då€‹elementã€‚æ¯å€‹batchéƒ½æŒ‰ç…§æ­¤æ–¹å¼åˆ†å¡Šï¼Œä¸”batchä¹‹é–“æ˜¯ç¨ç«‹çš„ã€‚

 Kèˆ‡Vå‰‡æ˜¯åœ¨æ¯å€‹blockå…§ï¼Œè¢«åŠƒåˆ†æˆå¤§å°ç‚º`TILE_SIZE * d`çš„tileï¼Œå…¶ä¸­TILE_SIZEå°±æ˜¯æ¼”ç®—æ³•ä¸­çš„B_cã€‚ä¸€å€‹blockè² è²¬Bræ¢qiçš„è¨ˆç®—ï¼Œæ¯æ¢qiåˆåˆ†æˆå¤šå€‹tileï¼Œæ¯å€‹tileå°±æ˜¯å‰›æ‰æåˆ°Kèˆ‡Vè¢«åŠƒåˆ†çš„å¤§å°ã€‚

### 3. Describe how you chose the block sizes B_r and B_c and why.

ç¸½å…±ç”¨åˆ°çš„shared memoryé‡ç‚ºkernel functionä¸­åˆ—å‡ºä¾†çš„ tile_k, tile_v, qi, oi, li, mi, sij, pijä¹‹ç¸½å’Œï¼Œä¸€å…±`2 * Bc * 64 + 2 * br * 64 + 2 * br + 2 * br * bc`ï¼Œå…¶ä¸­Br = 32, è€ŒBc = Br / 2ã€‚

æœƒå°‡Bcè¨­æˆBrçš„ä¸€åŠï¼Œæ˜¯ç‚ºäº†tilingè¨ˆç®—æ™‚èƒ½æ¸›å°‘æ¯å€‹tileæ‰€éœ€çš„shared memoryï¼Œå…è¨±æ›´å¤šblockåŒæ™‚åœ¨ä¸€å€‹SMä¸Šé¢åŸ·è¡Œã€‚å¦å¤–ï¼Œå°‘é‡çš„bcå¯ä»¥æé«˜æ•¸æ“šè¨ªå•çš„å±€éƒ¨æ€§ï¼Œä¸¦ä¸”æ¸›å°‘__syncthreads()æ™‚éœ€è¦åŒæ­¥çš„threadæ•¸é‡ã€‚

Brå‰‡æ˜¯è¨­æˆ32ï¼Œèˆ‡warp sizeå°é½Šï¼Œé”åˆ°occupancy optimizationçš„æ•ˆæœã€‚å¦‚æœè¨­æˆ64ï¼Œæ ¹æ“šä¸Šé¢åˆ—å‡ºçš„shared memory ä½¿ç”¨ç¸½å’Œè¨ˆç®—ï¼Œæœƒè¶…éshared memoryå¯ä»¥ä½¿ç”¨çš„è¨˜æ†¶é«”é™åˆ¶ã€‚

### 4.  Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.

* grid_dim

    ```cpp
    dim3 grid_dim((N + Br - 1)/Br, 1, B);
    ```

* #threads per block

    ```cpp
    dim3 block_dim(Br);  // Threads per block, Br = 32
    ```

* shared memory allocation

    ```cpp
    const int TILE_SIZE = Br/2;
    __shared__ float tile_k[TILE_SIZE * 64];
    __shared__ float tile_v[TILE_SIZE * 64];
    __shared__ float qi[Br * 64];
    __shared__ float oi[Br * 64];
    __shared__ float li[Br];
    __shared__ float mi[Br];
    __shared__ float sij[Br * TILE_SIZE];
    __shared__ float pij[Br * TILE_SIZE];
    ```
### 5. Justify your choices and how they relate to the blocking factors and the SRAM size.

é…ç½®çš„åŸå› å‰é¢å°±æéäº†ï¼Œæ­¤è™•ä¸è´…è¿°ã€‚ä¸¦ä¸” br è·Ÿ bc çš„è¨­ç½®å¸¶å…¥shared memory ç¸½ä½¿ç”¨é‡`2 * Bc * 64 + 2 * br * 64 + 2 * br + 2 * br * bc` = 28928 bytesï¼Œä¸æœƒè¶…éä¸€å€‹blockçš„shared memory é™åˆ¶ (49152 bytes)ã€‚

## Profiling Results

testcases : t20

![profile](https://github.com/109062227/Parallel-Programming/blob/main/hw4/profile.png?raw=true)

å¯ä»¥çœ‹åˆ°sm_effiencyé«˜é”90å¤šè¶´ï¼Œä»£è¡¨occupancy optimizationçš„éƒ¨åˆ†å¾ˆæˆåŠŸã€‚

## Experiment&Analysis

### 1.  System Spec

ä½¿ç”¨èª²å ‚æä¾›çš„Apollo server.

### 2. Optimization

testcases : t20

![optimization](https://github.com/109062227/Parallel-Programming/blob/main/hw4/optimization.png?raw=true)

Optimization çš„ method å’Œåšæ³•å¦‚å‰é¢ implementation ä¸­æ‰€æï¼Œå°é€Ÿåº¦å½±éŸ¿æœ€å¤§çš„æ˜¯shared memoryçš„ä½¿ç”¨ï¼Œå¯è¦‹memoryå­˜å–é€Ÿåº¦å°æ•´å€‹
runtime çš„å½±éŸ¿ä¹‹å¤§ã€‚Coalesced memoryå› ç‚ºåœ¨æ¯å€‹kernel block è®€å–Qã€Kã€VçŸ©é™£æ™‚ï¼Œä½¿ç”¨äº†é€£çºŒçš„memory access patternï¼Œæ‰€ä»¥å°runtimeä¹Ÿæœ‰äº›å¾®çš„å¹«åŠ©ã€‚Occupancy Optimizationå‰‡æ˜¯å› ç‚ºthreadè¨­ç½®æˆwarp sizeï¼Œä»¥åŠåˆç†çš„blockã€tileé…ç½®ï¼ŒæˆåŠŸæœ€å¤§åŒ–äº†æ¯å€‹SMçš„é‹ç®—èƒ½åŠ›(profilingä¸­å¯ä»¥ç™¼ç¾sm_efficiencyé«˜é”90å¹¾è¶´)ï¼Œæå‡åŸ·è¡Œæ•ˆç‡ã€‚

### Others

* Brè¨­ç½®

    testcases : t20

    å‚™è¨»ï¼šç‚ºäº†èƒ½å¤ ç¬¦åˆshared memoryä½¿ç”¨é™åˆ¶ï¼Œå°‡bcæš«æ™‚èª¿æˆbr/8ï¼Œé€™æ¨£æ‰èƒ½è·‘br = 64çš„caseã€‚

    ![br](https://github.com/109062227/Parallel-Programming/blob/main/hw4/diff_br.png?raw=true)

    å¯ä»¥ç™¼ç¾br = 64æ™‚è¡¨ç¾è¼ƒå·®ï¼Œå¯èƒ½æ˜¯å› ç‚ºæˆ‘æ²’æœ‰handle bank conflictï¼Œæ‰€ä»¥blocking factorèª¿è¶Šå¤§ï¼Œthreadsä¹‹é–“å¯èƒ½æœƒæ›´é »ç¹å»accessåŒå€‹bankï¼Œé€ æˆconflictã€‚

* Bcè¨­ç½®

    testcases : t20

    ![bc](https://github.com/109062227/Parallel-Programming/blob/main/hw4/diff_bc.png?raw=true)

    å˜—è©¦èª¿æ•´bcçš„å¤§å°ï¼ŒåŸå§‹è¨­å®šæ˜¯br / 2ã€‚xè»¸ä»£è¡¨bcæ”¹æˆbr/1, br/2, br/4...ä¾æ­¤é¡æ¨ï¼Œç™¼ç¾è¼ƒå¤§çš„bcæœƒé€ æˆè¡¨ç¾è¼ƒå·®ï¼Œbc = br/16æ™‚å°¤å…¶æ˜é¡¯ã€‚å› ç‚ºåˆ‡æˆè¶Šå¤šå¡Štileï¼Œå¯èƒ½æœƒå°è‡´æ•¸æ“šåœ¨global èˆ‡ shared memoryä¹‹é–“é »ç¹äº¤æ›ï¼Œå³memory accessä¸é«˜æ•ˆã€‚å¦å¤–ï¼Œè¼ƒå¤§çš„bcå¯èƒ½ä¹Ÿå°è‡´å¤§é‡åŒæ­¥åŒ–ç­‰å¾…æ™‚é–“ï¼Œé€ æˆæ•ˆç‡å·®ã€‚

    bc = br / 1å…¶å¯¦æ™‚é–“ç¨ç‚ºæ¯”br / 2ä½äº†ä¸€é»ï¼Œä½†è€ƒé‡åˆ°tileå¦‚æœåˆ‡å°ä¸€é»ï¼Œå¯ä»¥å¢åŠ sm_efficiencyï¼Œæ•…æœ€å¾Œä»é¸æ“‡ä½¿ç”¨br / 2ç•¶ä½œbcã€‚

* Different version implementation

    testcases : t22

    ![version](https://github.com/109062227/Parallel-Programming/blob/main/hw4/version.png?raw=true)

    - old version: grid_dim(B, 1), block_dim(32)çš„ç‰ˆæœ¬ï¼Œå¯èƒ½å› ç‚ºåœ¨è¨­è¨ˆkernel blockçš„éƒ¨åˆ†ä¸å¤ å¹³è¡ŒåŒ–ï¼Œåªæƒ³åˆ°ç”¨batchçš„æ•¸é‡ä¾†åˆ†å‰²ã€‚ä½†é€™æœƒå°è‡´åœ¨Bä¸å¤§ï¼ŒNå¾ˆå¤§çš„æ™‚å€™ç”¢ç”ŸTLEã€‚å¦‚æˆ‘ä½¿ç”¨çš„t22 testcases, å¦å¤–t26~t30ä¹Ÿéƒ½æœƒæœ‰TLEçš„ç‹€æ³ã€‚

    - new version: æœ€å¾Œå¯¦ä½œçš„æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯å‰é¢æœ‰ä»‹ç´¹åˆ°çš„grid_dimèˆ‡block_dimåˆ‡æ³•ã€‚

## Experience & conclusion

### 1. What have you learned from this homework?

åŸä¾†è¨­è¨ˆgrid_dimèˆ‡block_diméœ€è¦é€™éº¼å¤šå·§æ€ï¼ŒçœŸçš„è¦æ³¨æ„å¾ˆå¤šçœ‰è§’ï¼Œåˆ‡å¾—ä¸å¥½å¯èƒ½æœƒTLEã€‚æˆ‘ä¹Ÿå°æ–¼GPUçš„æ’°å¯«æœ‰æ›´æ·±åˆ»çš„èªè­˜ï¼Œå„ç¨®å„ªåŒ–æ–¹æ³•è‡ªå·±å¯¦ä½œå¾Œæ¯”ä¸Šèª²å–®ç´”è½è¬›æ›´æœ‰æ¦‚å¿µã€‚

### 2. Feedback

è¬è¬æ•™æˆã€åŠ©æ•™ï¼Œä½ å€‘è¾›è‹¦äº†ï¼